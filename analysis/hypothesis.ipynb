{"cells":[{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from math import log\n","from sklearn.model_selection import train_test_split\n","from scipy import stats\n","from sklearn.linear_model import Ridge\n","\n","import sys\n","sys.path.append('../utils')\n","from multiple_regression import *\n","from round_result import round_result\n"]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fixed acidity</th>\n","      <th>volatile acidity</th>\n","      <th>citric acid</th>\n","      <th>residual sugar</th>\n","      <th>chlorides</th>\n","      <th>free sulfur dioxide</th>\n","      <th>total sulfur dioxide</th>\n","      <th>density</th>\n","      <th>pH</th>\n","      <th>sulphates</th>\n","      <th>alcohol</th>\n","      <th>quality</th>\n","      <th>const</th>\n","      <th>alcohol_log</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7.0</td>\n","      <td>0.27</td>\n","      <td>0.36</td>\n","      <td>20.7</td>\n","      <td>0.045</td>\n","      <td>45.0</td>\n","      <td>170.0</td>\n","      <td>1.00100</td>\n","      <td>3.00</td>\n","      <td>0.45</td>\n","      <td>8.8</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>2.174752</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>6.3</td>\n","      <td>0.30</td>\n","      <td>0.34</td>\n","      <td>1.6</td>\n","      <td>0.049</td>\n","      <td>14.0</td>\n","      <td>132.0</td>\n","      <td>0.99400</td>\n","      <td>3.30</td>\n","      <td>0.49</td>\n","      <td>9.5</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>2.251292</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8.1</td>\n","      <td>0.28</td>\n","      <td>0.40</td>\n","      <td>6.9</td>\n","      <td>0.050</td>\n","      <td>30.0</td>\n","      <td>97.0</td>\n","      <td>0.99510</td>\n","      <td>3.26</td>\n","      <td>0.44</td>\n","      <td>10.1</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>2.312535</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7.2</td>\n","      <td>0.23</td>\n","      <td>0.32</td>\n","      <td>8.5</td>\n","      <td>0.058</td>\n","      <td>47.0</td>\n","      <td>186.0</td>\n","      <td>0.99560</td>\n","      <td>3.19</td>\n","      <td>0.40</td>\n","      <td>9.9</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>2.292535</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7.2</td>\n","      <td>0.23</td>\n","      <td>0.32</td>\n","      <td>8.5</td>\n","      <td>0.058</td>\n","      <td>47.0</td>\n","      <td>186.0</td>\n","      <td>0.99560</td>\n","      <td>3.19</td>\n","      <td>0.40</td>\n","      <td>9.9</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>2.292535</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4893</th>\n","      <td>6.2</td>\n","      <td>0.21</td>\n","      <td>0.29</td>\n","      <td>1.6</td>\n","      <td>0.039</td>\n","      <td>24.0</td>\n","      <td>92.0</td>\n","      <td>0.99114</td>\n","      <td>3.27</td>\n","      <td>0.50</td>\n","      <td>11.2</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>2.415914</td>\n","    </tr>\n","    <tr>\n","      <th>4894</th>\n","      <td>6.6</td>\n","      <td>0.32</td>\n","      <td>0.36</td>\n","      <td>8.0</td>\n","      <td>0.047</td>\n","      <td>57.0</td>\n","      <td>168.0</td>\n","      <td>0.99490</td>\n","      <td>3.15</td>\n","      <td>0.46</td>\n","      <td>9.6</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>2.261763</td>\n","    </tr>\n","    <tr>\n","      <th>4895</th>\n","      <td>6.5</td>\n","      <td>0.24</td>\n","      <td>0.19</td>\n","      <td>1.2</td>\n","      <td>0.041</td>\n","      <td>30.0</td>\n","      <td>111.0</td>\n","      <td>0.99254</td>\n","      <td>2.99</td>\n","      <td>0.46</td>\n","      <td>9.4</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>2.240710</td>\n","    </tr>\n","    <tr>\n","      <th>4896</th>\n","      <td>5.5</td>\n","      <td>0.29</td>\n","      <td>0.30</td>\n","      <td>1.1</td>\n","      <td>0.022</td>\n","      <td>20.0</td>\n","      <td>110.0</td>\n","      <td>0.98869</td>\n","      <td>3.34</td>\n","      <td>0.38</td>\n","      <td>12.8</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>2.549445</td>\n","    </tr>\n","    <tr>\n","      <th>4897</th>\n","      <td>6.0</td>\n","      <td>0.21</td>\n","      <td>0.38</td>\n","      <td>0.8</td>\n","      <td>0.020</td>\n","      <td>22.0</td>\n","      <td>98.0</td>\n","      <td>0.98941</td>\n","      <td>3.26</td>\n","      <td>0.32</td>\n","      <td>11.8</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>2.468100</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4898 rows × 14 columns</p>\n","</div>"],"text/plain":["      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n","0               7.0              0.27         0.36            20.7      0.045   \n","1               6.3              0.30         0.34             1.6      0.049   \n","2               8.1              0.28         0.40             6.9      0.050   \n","3               7.2              0.23         0.32             8.5      0.058   \n","4               7.2              0.23         0.32             8.5      0.058   \n","...             ...               ...          ...             ...        ...   \n","4893            6.2              0.21         0.29             1.6      0.039   \n","4894            6.6              0.32         0.36             8.0      0.047   \n","4895            6.5              0.24         0.19             1.2      0.041   \n","4896            5.5              0.29         0.30             1.1      0.022   \n","4897            6.0              0.21         0.38             0.8      0.020   \n","\n","      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n","0                    45.0                 170.0  1.00100  3.00       0.45   \n","1                    14.0                 132.0  0.99400  3.30       0.49   \n","2                    30.0                  97.0  0.99510  3.26       0.44   \n","3                    47.0                 186.0  0.99560  3.19       0.40   \n","4                    47.0                 186.0  0.99560  3.19       0.40   \n","...                   ...                   ...      ...   ...        ...   \n","4893                 24.0                  92.0  0.99114  3.27       0.50   \n","4894                 57.0                 168.0  0.99490  3.15       0.46   \n","4895                 30.0                 111.0  0.99254  2.99       0.46   \n","4896                 20.0                 110.0  0.98869  3.34       0.38   \n","4897                 22.0                  98.0  0.98941  3.26       0.32   \n","\n","      alcohol  quality  const  alcohol_log  \n","0         8.8        6      1     2.174752  \n","1         9.5        6      1     2.251292  \n","2        10.1        6      1     2.312535  \n","3         9.9        6      1     2.292535  \n","4         9.9        6      1     2.292535  \n","...       ...      ...    ...          ...  \n","4893     11.2        6      1     2.415914  \n","4894      9.6        5      1     2.261763  \n","4895      9.4        6      1     2.240710  \n","4896     12.8        7      1     2.549445  \n","4897     11.8        6      1     2.468100  \n","\n","[4898 rows x 14 columns]"]},"execution_count":123,"metadata":{},"output_type":"execute_result"}],"source":["wine = pd.read_csv('../datasets/winequality-white.csv', sep=';')\n","wine['const'] = 1\n","wine['alcohol_log'] = wine['alcohol'].apply(lambda x: log(x))\n","\n","X_train, X_test, y_train, y_test = train_test_split(wine.drop('quality', axis=1), wine['quality'], test_size=0.2, random_state=0)\n","wine"]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[{"data":{"text/plain":["count    980.000000\n","mean       5.821429\n","std        0.939296\n","min        3.000000\n","25%        5.000000\n","50%        6.000000\n","75%        6.000000\n","max        8.000000\n","Name: quality, dtype: float64"]},"execution_count":124,"metadata":{},"output_type":"execute_result"}],"source":["y_test.describe()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Zanim przejdziemy do pracy nad modelem, obserwujemy zakresy uzyskane przez podział danych."]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[],"source":["hypothesis_features = ['const', 'alcohol', 'volatile acidity', 'density', 'alcohol_log']\n","considered_features = []\n","\n","test_results = []"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Będziemy iterować po kolejnych cechach z hipotezy, dodając je kolejno do modelu i obserwując zachodzące w nim zmiany."]},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Added: const\n","\tAvg train error =\t0.7582104012683419\n","\tAvg validate error =\t0.7616538590496968\n","\tTest error =\t\t0.886363066388164\n","\n","        y_pred\n","2762  5.892037\n","42    5.892037\n","1419  5.892037\n","3664  5.892037\n","2125  5.892037\n","...        ...\n","2111  5.892037\n","1828  5.892037\n","1256  5.892037\n","3335  5.892037\n","230   5.892037\n","\n","[980 rows x 1 columns]\n"]},{"data":{"text/plain":["count    3918.000000\n","mean        5.892037\n","std         0.871254\n","min         3.000000\n","25%         5.000000\n","50%         6.000000\n","75%         6.000000\n","max         9.000000\n","Name: quality, dtype: float64"]},"execution_count":126,"metadata":{},"output_type":"execute_result"}],"source":["# const\n","i = 0\n","considered_features.append(hypothesis_features[i])\n","test_results.append(describe_multiple_regression_model(X_train, X_test, y_train, y_test, considered_features, hypothesis_features[i]))\n","\n","y_pred = pd.DataFrame(test_results[i][1], index=y_test.index, columns=[\"y_pred\"])\n","print(y_pred)\n","y_train.describe()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Na początku, jako próbę kontrolną, wykorzystamy jedynie wyraz wolny.  \n","Obserwujemy, że dla każdego rekordu została przypisana wartość średnia jakości ze zbioru treningowego.\n","\n","Pomimo prymitywności tego modelu, okazał się on zaskakująco skuteczny - otrzymaliśmy błąd średniokwadratowy mniejszy od 1 - w przybliżeniu 0.886. Wynika to ze słabego zbalansowania badanego zbioru - jak wcześniej wspominaliśmy bardzo mało jest tu win o skrajnych jakościach. Jest to na ogół zgodne z intuicją - trudno jest stworzyć produkt bliski perfekcji, jak również żadna szanująca się firma nie chce sprzedawać win ocenianych jako najgorsze na rynku. Stąd zapewne ilościowa tendencja w danych do średniej jakości.\n","\n","Sugeruje to również, że gdy brak nam doświadczenia czy odpowiedniego modelu, strzał że wino które właśnie otwieramy będzie średniej jakości ma spore szane się sprawdzić, albo przynajmniej nie powinniśmy pomylić się znacząco."]},{"cell_type":"code","execution_count":127,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Added: alcohol\n","\tAvg train error =\t0.610666786267788\n","\tAvg validate error =\t0.6166589110158853\n","\tTest error =\t\t0.7306442340192554\n","\n","        y_pred\n","3265  5.108744\n","294   5.264247\n","867   5.264247\n","4014  5.264247\n","3420  5.264247\n","...        ...\n","3086  6.881477\n","3083  6.881477\n","3915  6.974779\n","3150  6.974779\n","3904  6.974779\n","\n","[980 rows x 1 columns]\n"]}],"source":["# const, alcohol\n","i += 1\n","considered_features.append(hypothesis_features[i])\n","test_results.append(describe_multiple_regression_model(X_train, X_test, y_train, y_test, considered_features, hypothesis_features[i]))\n","\n","print(pd.DataFrame(test_results[i][1], index=y_test.index, columns=[\"y_pred\"]).sort_values(by=\"y_pred\"))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Dodanie do modelu stężenia alkoholu jako cechy decyzyjnej zwiększyło zarówno zakres (pomiędzy max a min jest teraz prawie 2 różnicy) jak i dokładność wyników (zarówno przy treningu jak i testach o około 0.15 (rms)). Jednak model wciąż musi poszerzyć zakres wyników, by móc uzyskać lub chociaż zbliżyć się do wartości granicznych."]},{"cell_type":"code","execution_count":128,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Added: volatile acidity\n","\tAvg train error =\t0.5730409489874573\n","\tAvg validate error =\t0.5750177189933506\n","\tTest error =\t\t0.6866714897604801\n","\n","        y_pred\n","1951  4.366818\n","2154  4.513727\n","294   4.635631\n","230   4.665712\n","4619  4.704887\n","...        ...\n","3150  6.787668\n","4645  6.793441\n","3086  6.848356\n","3083  6.848356\n","2814  6.859376\n","\n","[980 rows x 1 columns]\n"]}],"source":["# const, alcohol, volatile acidity\n","i += 1\n","considered_features.append(hypothesis_features[i])\n","test_results.append(describe_multiple_regression_model(X_train, X_test, y_train, y_test, considered_features, hypothesis_features[i]))\n","\n","print(pd.DataFrame(test_results[i][1], index=y_test.index, columns=[\"y_pred\"]).sort_values(by=\"y_pred\"))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Poprzez dodanie cechy \"volatile acidity\" do modelu udało się poszerzyć znacząco dolny zakres (oraz tak jak poprzednio zmniejszyć błąd, ale tym razem już o około 0.05).\n","Jest to więc zdecydowanie wartościowa zmiana, co jest zgodne z oczekiwaniami - jest to cecha u której zaobserwowaliśmy korelację z jakością, oraz brak korelacji ze stężeniem alkoholu, który to został dodany już do modelu. Zatem lotna kwasowość ma na model wpływ niepokrywający się z wpływem alkoholu. Jednak wciąż nie udało się sięgnąć granicznych wartości z danych testowych."]},{"cell_type":"code","execution_count":129,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Added: density\n","\tAvg train error =\t0.568105701666267\n","\tAvg validate error =\t0.5665895047926881\n","\tTest error =\t\t0.6837200879145863\n","\n","        y_pred\n","1951  4.403961\n","294   4.568438\n","4619  4.615174\n","230   4.645602\n","2154  4.677024\n","...        ...\n","3500  6.891752\n","3086  6.895834\n","3083  6.895834\n","2781  6.922634\n","3150  6.925576\n","\n","[980 rows x 1 columns]\n","Added: density, removed alcohol\n","\tAvg train error =\t0.6528045947721773\n","\tAvg validate error =\t0.6620605779249871\n","\tTest error =\t\t0.7981522226486801\n","\n"]}],"source":["# const, alcohol, volatile acidity, density\n","i += 1\n","considered_features.append(hypothesis_features[i])\n","test_results.append(describe_multiple_regression_model(X_train, X_test, y_train, y_test, considered_features, hypothesis_features[i]))\n","\n","print(pd.DataFrame(test_results[i][1], index=y_test.index, columns=[\"y_pred\"]).sort_values(by=\"y_pred\"))\n","\n","# const, volatile acidity, density\n","describe_multiple_regression_model(X_train, X_test, y_train, y_test, ['const', 'volatile acidity', 'density'], \"density, removed alcohol\")\n","pass"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Dodanie do modelu atrybutu gęstości przyczyniło się do wzrostu jakości rzędu mniejszego niż poprzednio, bo tylko 0.003. Oznacza to, że błędnym było założenie reprezentacji przez gęstość pozostałych cech, równoważących się z alkoholem. Również pozbycie się z modelu alkoholu na rzecz gęstości spowodowało znaczący spadek jakości, zatem jest ona na ogół raczej słabym reprezentantem jakości wina."]},{"cell_type":"code","execution_count":130,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Added: alcohol_log\n","\tAvg train error =\t0.5598930122693325\n","\tAvg validate error =\t0.5578681456376386\n","\tTest error =\t\t0.6824533511240435\n","\n","        y_pred\n","1951  4.170298\n","2154  4.428696\n","4619  4.702256\n","230   4.712467\n","294   4.780159\n","...        ...\n","3904  7.082939\n","3514  7.114153\n","3083  7.250267\n","3086  7.250267\n","3150  7.333846\n","\n","[980 rows x 1 columns]\n"]}],"source":["# const, alcohol, volatile acidity, density, alcohol_log\n","i += 1\n","considered_features.append(hypothesis_features[i])\n","test_results.append(describe_multiple_regression_model(X_train, X_test, y_train, y_test, considered_features, hypothesis_features[i]))\n","\n","print(pd.DataFrame(test_results[i][1], index=y_test.index, columns=[\"y_pred\"]).sort_values(by=\"y_pred\"))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Po sprawdzeniu cech zadanych w hipotezie, postanowiliśmy sprawdzić jeszcze pewną wariację odnośnie jednej z jej cech. Mianowicie zdecyfowaliśmy się zniwelować prawoskośność wykresu alkoholu poprzez dodanie cechy zlogarytmowanej wartości stężenia alkoholu.\n","\n","Choć wynikowy zysk jakości był marginalny, znacząco poszerzerzył się zakres wyników, co oznacza większą na ogół większą zdolność modelu do przyjmowania bardziej rozproszonych od średniej danych."]},{"cell_type":"code","execution_count":131,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Added: const (lambda=49.152)\n","    \tAvg train error =\t0.7664878521630156\n","    \tAvg validate error =\t0.7699424034908015\n","    \tTest error =\t\t0.8813832757869122\n","    \tTest error gain =\t0.0049797906012517545\n","    \n","Added: alcohol (lambda=49.152)\n","    \tAvg train error =\t0.6189826446907183\n","    \tAvg validate error =\t0.6244869050264605\n","    \tTest error =\t\t0.7265834331392841\n","    \tTest error gain =\t0.004060800879971294\n","    \n","Added: volatile acidity (lambda=49.152)\n","    \tAvg train error =\t0.5813728819130781\n","    \tAvg validate error =\t0.5825860825494514\n","    \tTest error =\t\t0.6826376071866341\n","    \tTest error gain =\t0.004033882573845959\n","    \n","Added: density (lambda=49.152)\n","    \tAvg train error =\t0.5766039120286803\n","    \tAvg validate error =\t0.575227268830109\n","    \tTest error =\t\t0.6782744784691089\n","    \tTest error gain =\t0.00544560944547734\n","    \n","Added: alcohol_log (lambda=49.152)\n","    \tAvg train error =\t0.577230141455302\n","    \tAvg validate error =\t0.5758812913259181\n","    \tTest error =\t\t0.6784588821515019\n","    \tTest error gain =\t0.003994468972541632\n","    \n"]}],"source":["hypothesis_features = ['const', 'alcohol', 'volatile acidity', 'density', 'alcohol_log']\n","considered_features = []\n","\n","min_lambda = 0.003\n","lambda_multiplier_step = 2\n","max_lambda = 150\n","\n","best_results = []\n","\n","for i, feature in enumerate(hypothesis_features):\n","\n","    considered_features.append(feature)\n","    X_train_chosen = X_train[considered_features]\n","\n","    current_lambda = min_lambda\n","    best_result = {'best_lambda': 0.00001, 'avg_train_error': 9999, 'avg_val_error': 9999, 'test_error': 9999, 'test_predicted': None}\n","\n","    while current_lambda < max_lambda:\n","\n","        ridge_model = Ridge(alpha=current_lambda, fit_intercept=False)\n","        X_train_scaled = standard_scaled(X_train_chosen, True)\n","\n","        (avg_train_error, avg_val_error) = cross_validate(ridge_model, X_train_scaled, y_train)\n","        ridge_model.fit(X_train_scaled, y_train)\n","        test_predicted = ridge_model.predict(standard_scaled(X_test[considered_features], True))\n","        test_error = round_result(y_test, test_predicted)[1]['actual_mean_sqrt']\n","\n","        if test_error < best_result['test_error']:\n","            best_result['best_lambda'] = current_lambda\n","            best_result['avg_train_error'] = avg_train_error\n","            best_result['avg_val_error'] = avg_val_error\n","            best_result['test_error'] = test_error\n","            best_result['test_predicted'] = test_predicted\n","\n","        current_lambda *= lambda_multiplier_step\n","\n","    best_results.append(best_result)\n","    \n","    print(f\"\"\"Added: {feature} (lambda={best_result['best_lambda']})\n","    \\tAvg train error =\\t{best_result['avg_train_error']}\n","    \\tAvg validate error =\\t{best_result['avg_val_error']}\n","    \\tTest error =\\t\\t{best_result['test_error']}\n","    \\tTest error gain =\\t{test_results[i][0] - best_result['test_error']}\n","    \"\"\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Finalnie skorzystaliśmy z regresji grzbietowej, by możliwie zwiększyć zdolności modelu do znajdywania rozwiązań mniej skojarzonych z konkretnym zbiorem uczącym. Jednak ostateczny zysk w postaci mniejszego błędu testowego rms był stosunkowo niewielki - na poziomie około 0.005.\n","\n","Jak widzimy poniżej, również zakresy zostały zawężone. Ale być może właśnie to skupienie bliżej średniej pozwala lepiej dopasowywać się do nieznanych danych?"]},{"cell_type":"code","execution_count":133,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["        y_pred\n","1951  4.378749\n","294   4.518549\n","4619  4.576753\n","230   4.594193\n","2154  4.620189\n","...        ...\n","3500  6.789741\n","2814  6.794228\n","3083  6.830161\n","3086  6.830161\n","3150  6.847046\n","\n","[980 rows x 1 columns]\n"]}],"source":["print(pd.DataFrame(best_results[4]['test_predicted'], index=y_test.index, columns=[\"y_pred\"]).sort_values(by=\"y_pred\"))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"b75619628fa9cf4dd1a004fda5ca2dccf298a07641ec64ea527c0d4c1f1fee46"}}},"nbformat":4,"nbformat_minor":4}
